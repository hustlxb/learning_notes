# 学习笔记

## XGboost
***
参考
[机器学习集成算法：XGBoost思想](https://mp.weixin.qq.com/s?__biz=MzI3NTkyMjA4NA==&mid=2247484457&idx=1&sn=d888053d33d6db465f8a0c5d9b620c27&chksm=eb7c29e2dc0ba0f4ddac4764ca5e475b50bb7866df0d11e6cb5de82dac3b938b999bc4ac5500&scene=21#wechat_redirect)

## numpy
17NumPy User Guide, Release 1.14.5
>2.4. Copies and Views  
2.4.4 Functions and Methods Overview  
***
Here is a list of some useful NumPy functions and methods names ordered in categories. See routines for the full list.

>Array Creation 
>>arange, array, copy, empty, empty_like, eye, fromfile, fromfunction,  
identity, linspace, logspace, mgrid, ogrid, ones, ones_like, r, zeros, zeros_like

>Conversions 
>>ndarray.astype, atleast_1d, atleast_2d, atleast_3d, mat

>Manipulations 
>>array_split, column_stack, concatenate, diagonal, dsplit, dstack, hsplit,
hstack, ndarray.item, newaxis, ravel, repeat, reshape, resize, squeeze, swapaxes,
take, transpose, vsplit, vstack

>Questions
>>all, any, nonzero, where

>Ordering 
>>argmax, argmin, argsort, max, min, ptp, searchsorted, sort

>Operations 
>>choose, compress, cumprod, cumsum, inner, ndarray.fill, imag, prod, put, putmask,real, sum

>Basic Statistics 
>>cov, mean, std, var

>Basic Linear Algebra 
>>cross, dot, outer, linalg.svd, vdot


## pandas 
[案例实战|泰坦尼克号船员获救预测（数据预处理部分）
](https://mp.weixin.qq.com/s?__biz=MzI3NTkyMjA4NA==&mid=2247484736&idx=1&sn=37ebdb7ea28bfc081e0bb683c7def26e&chksm=eb7c288bdc0ba19d6b2e9f9c5c0c51ddaf20802e30343273fa706dfeee9de23090200a54550a&scene=21#wechat_redirect)
***
>数据清洗
```python
full_data = [train, test]

for dataset in full_data:

    age_avg = dataset['Age'].mean()

    age_std = dataset['Age'].std()

    age_null_count = dataset['Age'].isnull().sum()

    age_null_random_list = np.random.randint(age_avg - age_std, age_avg + age_std, size=age_null_count)

    dataset['Age'][np.isnan(dataset['Age'])] = age_null_random_list

#Cannot convert non-finite values (NA or inf) to integer，因此需要先做Na检查

    dataset['Age'] = dataset['Age'].astype(int)
```
## 多版本python混合使用
使用python的一些个人经验,linux同时使用python2 和 pyhon3
### pip 安装
> python2  
```bash
sudo apt-get install python-pip
```  
> python3  
```
sudo apt-get install python3-pip
```
#### 查看python版本和路径
```
>pip -V or pip2 -V or python2 -m pip -V
pip 9.0.1 from /usr/lib/python2.7/dist-packages (python 2.7)
>pip3 -V or python3 -m pip -V
pip 9.0.1 from /usr/lib/python3/dist-packages (python 3.6)

>python2
Python 2.7.15rc1 (default, Apr 15 2018, 21:51:34) 
[GCC 7.3.0] on linux2
Type "help", "copyright", "credits" or "license" for more information.
>>> 

>python3
Python 3.6.5 (default, Apr  1 2018, 05:46:30) 
[GCC 7.3.0] on linux
Type "help", "copyright", "credits" or "license" for more information.
>>> 

```

### 给特定的python环境安装库
> python2  
```bash
python2 -m pip install package-name  # python2 为python2版本指令
```
>python3
```bash
python3 -m pip install package-name  # python3 为python3版本指令
```

### 使用anaconda
可以使用anaconda实现多个python环境共存
#### 安装
```bash
# 获取安装包
sudo wget https://mirrors.tuna.tsinghua.edu.cn/anaconda/archive/Anaconda3-5.1.0-Linux-x86_64.sh
# 安装
sudo ./https://mirrors.tuna.tsinghua.edu.cn/anaconda/archive/Anaconda3-5.1.0-Linux-x86_64.sh
```
### 在环境变量中配置anaconda
```bash
export PATH="/home/***/anaconda3/bin:$PATH"
# 起个别名
alias python36="/usr/bin/python3.6"
alias python27="/usr/bin/python2.7"
# anaconda装了多个版本环境,也可以在这里多配置几个别名.
```
此时,系统中python为anaconda的python版本,python2和python3为系统自带的python,这也是改名的意义所在了~
#### 再查看一下pip路径
```bash
> pip -V #anaconda目前所使用环境的pip 
pip 10.0.1 from /home/***/anaconda3/lib/python3.6/site-packages/pip (python 3.6)

> python2 -m pip -V # 如果anaconda装了python2版本的话
pip 10.0.1 from /home/xuebin/anaconda3/envs/python27/lib/python2.7/site-packages/pip (python 2.7)

>pip2 -V or python27 -m pip -V
pip 9.0.1 from /usr/lib/python2.7/dist-packages (python 2.7)

>pip3 -V
pip 10.0.1 from /home/xuebin/anaconda3/lib/python3.6/site-packages/pip (python 3.6)

>python36 -m pip -V
pip 9.0.1 from /usr/lib/python3/dist-packages (python 3.6)
```
使用pip安装库的时候可以先查看一下,以免安装错误.
#### anaconda安装程序
>换源
```bash
conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free/
conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/main/
conda config --set show_channel_urls yes
```
> 安装所需库,例如
```bash
conda install pandas
conda install pip
# 安装后pip默认为Python3.6版本
pip install keras sklearn distributed
pip install pyqt5
sudo apt install libgl1-mesa-glx
# 下载stomp.py
cd /mnt/software
wget https://github.com/jasonrbriggs/stomp.py/archive/master.zip
unzip unzip stomp.py-master.zip 
# cd stomp.py-master/

make install

# 将项目目录添加到环境中
sudo echo '/mnt/python/ML' > /root/anaconda3/lib/python3.6/site-packages/ml_module.pth
```

#### 常用anaconda 命令
```bash
conda -h

# 查看已配置环境
conda info -e  
# 激活某个环境,激活后安装操作就是在当前环境下了
source activate python36
# 取消激活  
source deactivate
```

## Supervisor-进程监控自动重启
本次使用supervisor来监控服务器的python程序
supervisor只支持python2, 可以使用python2安装,用来监控python3
### 安装
> 安装python2 pip
```bash
sudo apt-get install python-pip
# python3 使用 sudo apt-get install python3-pip
```
> 安装supervisor
```
# 指定用python2版本安装
python2 -m pip install supervisor # pyhon2应换为自己电脑上面的python版本指令
```
>  supervisor主要有两个配置文件和两个命令
> 配置文件  surpvisord.conf 和 app_name.ini
> 命       令  supervisord 和 supervisorctl 
### 配置文件
> 生成配置文件
```bash
# su 进入root模式下 
# 使用sudo echo_supervisord_conf > /etc/supervisor/supervisord.conf 会有权限错误
echo_supervisord_conf > /etc/supervisor/supervisord.conf
# 或者使用
sudo su - root -c "echo_supervisord_conf > /etc/supervisor/supervisord.conf"
```

> 修改配置文件 主要修改两个部分 ;是注释  
> 参考[Supervisor Configure](http://supervisord.org/configuration.html)
> 
* include 用来配置要存放的程序配置
    ``` 
    [include]
    files = /etc/supervisor/*.ini
    ```
* unix_http_server
    ```
    [unix_http_server]
    file=/tmp/supervisor.sock   ; the path to the socket file
    chmod=0766                 ; socket file mode (default 0700)
    ;chown=nobody:nogroup       ; socket file uid:gid owner
    ;username=user              ; default is no username (open server)
    ;password=123               ; default is no password (open server)
    ```
    
>要监控的程序在include配置的目录下配置:  
>
* app_name.ini 
    ```ini 
    [program:app_name]     # 程序名字
    directroy=/mnt/python/py_file/   # 程序所在的主目录
    command=/root/anaconda3/bin/python /mnt/python/py_file/%(program_name)s.py  # 执行程序的命令  
    autostart=true   # 是否自动启动程序
    startsecs=5      # 程序启动5秒种没有中断默认启动成功
    autorestart=true # 是否重启
    startretries=3   # 重启失败尝试次数
    user=root        # 执行程序的用户
    stdout_logfile=/mnt/python/log/%(program_name)s.log
    stderr_logfile=/mnt/python/log/%(program_name)s.log
    ```
### 启动 supervisor
```bash
sudo supervisord

# 没有使用 -c 制定配置文件时,默认从以下位置加载配置文件

$CWD/supervisord.conf
$CWD/etc/supervisord.conf
/etc/supervisord.conf
/etc/supervisor/supervisord.conf (since Supervisor 3.3.0)
../etc/supervisord.conf (Relative to the executable)
../supervisord.conf (Relative to the executable)

```
> 可以在日志中查看启动日志和错误日志, 或者通过supervisorctl查看进行运行情况


### supervisorctl的使用 
>可以通过```sudo supervisorctl [options] [action [arguments]] ```使用, 例:
```bash
sudo supervisorctl restart app_name
```
> 也可以在命令窗口中直接执行, 例:
```bash
sudo supervisorctl  # 进入命令窗口 
# 输出
app_name    RUNNING   pid 22364, uptime 0:02:23  # 进程运行情况
>                                                # 命令窗口

>restart app_name   # 重启app_name进程  
>restart all        # 重启全部进程
```
> supervisorctl常用命令
```bash
### 当配置文件改变时, 重新启动supervisor
> reread
# 或者
> reload
Really restart the remote supervisord process y/N? 
>y     

### 停止supervisord
sudo supervisorctl shutdown
### 停止进程
sudo supervisorctl stop app_name
### 停止全部进程
sudo supervisorctl stop all
```
### 其他命令的使用
> 查看superbisord帮助文档
```
sudo supervisord --help
```

> 查看superbisorctl帮助文档
```bash
> sudo supervisorctl --help

Options:
-c/--configuration FILENAME -- 配置文件路径 (没有则按照默认规则查找)
-h/--help -- 输出使用帮助并退出
-i/--interactive -- 执行命令后启动内置的交互命令窗口
-s/--serverurl URL -- supervisord服务监听的网址 (默认 "http://localhost:9001").
-u/--username USERNAME -- 用于服务器验证的用户
-p/--password PASSWORD -- 用于服务器验证的密码
-r/--history-file -- keep a readline history (if readline is available)

> sudo supervisorctl help

default commands (type help <topic>):
=====================================
add    exit      open  reload  restart   start   tail   
avail  fg        pid   remove  shutdown  status  update 
clear  maintail  quit  reread  signal    stop    version

# 命令帮助查看
> sudo supervisorctl help start

start <name>		Start a process
start <gname>:*		Start all processes in a group
start <name> <name>	Start multiple processes or groups
start all		Start all processes
```


## 强化学习
  
### Q-learning 算法

蒙特卡洛

## python log日志

### 自带logging库

```python
import loggging

# 设置log级别  INFO DEBUG
logging.basicConfig(level=logging.INFO)

```

## Markdown

>Markdown基本语法主要分为如下几大部分： 标题，段落，区块引用，代码区块，强调，列表，分割线，链接，图片，反斜杠 \，符号'`'

### 基本语法

**1 标题**

两种形式：

1）使用=和-标记一级和二级标题。

一级标题
=========
二级标题
---------

2）使用#，可表示1-6级标题。

# 一级标题
## 二级标题
### 三级标题
#### 四级标题
##### 五级标题
###### 六级标题

**2 段落**

段落的前后要有空行，所谓的空行是指没有文字内容。若想在段内强制换行的方式是使用两个以上空格加上回车（引用中换行省略回车）。  

**3 区块引用**

在段落的每行或者只在第一行使用符号>,还可使用多个嵌套引用，如：
> 区块引用
>> 嵌套引用

**4 代码区块**

代码区块的建立是在每行加上4个空格或者一个制表符（即保留代码格式）。如

```
void main()
{
printf("Hello, Markdown.");
}
```

**5 强调**

在强调内容两侧分别加上*或者_，如：

>*斜体*，_斜体_

>**粗体**，__粗体__

**6 列表**

使用·、+、或-标记无序列表，如：

>- 第一项 
>+ 第二项 
>* 第三项

注意：标记后面最少有一个_空格_或_制表符_。若不在引用区块中，必须和前方段落之间存在空行

有序列表的标记方式是将上述的符号换成数字,并辅以.，如：
>1. 第一项
>2. 第二项
>3. 第三项

**7 分割线**

分割线最常使用就是三个或以上*，还可以使用-和_

>***
>---
>___

**8 链接**

链接可以由两种形式生成：行内式和参考式。

行内式：

>[younghz的Markdown库](https:://github.com/younghz/Markdown "Markdown")

参考式：

[younghz的Markdown库1][1]

[younghz的Markdown库2][2]

[1]:https:://github.com/younghz/Markdown "Markdown"
[2]:https:://github.com/younghz/Markdown "Markdown"

（以上即为我的参考源）

**9 图片**

添加图片的形式和链接相似，只需在链接的基础上前方加一个'！'

![海伦罗德里格斯特里亚斯诞辰89周年](https://i2.wp.com/www.wailian.work/images/2018/07/07/imageb3cd3.png?zoom=1.2999999523162842&w=810&ssl=1)
(正好看到谷歌首页推这个)

**10 反斜杠**

\ 相当于反转义作用,使符号成为普通符号,与代码中使用方式相同
 
**11. 符号 '`'**

起到标记作用。如：

`ctrl+a`

```
三个```即为引用区块
```
### 其他语法

**12. 表**
                                   
|    a    |       b       |      c     |
|:-------:| :-------------| ----------:|
|   居中   |     左对齐     |   右对齐    |
|=========|===============|============|

**13 在文章最后面显示脚注**

Markdown[^1]

[^1]: Markdown是一种纯文本标记语言    
    


## 正则表达式

https://docs.python.org/3.6/library/re.html
```
ordinary characters: 'A','a' or '0' and so on
special characters: '|' '('
Repetition qualifiers(重复限定符)： (*, +, ?, {m,n})

. 匹配任何一个字符 除了新行
^ 匹配字符串开头
$ 匹配字符串结尾

* 匹配0或多个重复字符 ab* --> a ab abb abbb abbbb
+ 匹配1或多个重复字符 ab+ -->   ab abb abbb abbbb
? 匹配0或1个字符     ab? --> a ab

*?, +?, ??  非贪婪(non-greedy) 尽可能少的匹配   
            <.*?> --> '<a>'  而 <.*> --> '<a> b <c>'

{m} 匹配m个重复字符   a{2} --> aa  a{6} --> aaaaaa 
{m,n} 匹配m-n个重复字符  a{2,4} --> aa, aaa, aaaa
{m,n}? 非贪婪(non-greedy) 尽可能少的匹配 a{2,4} --> aa

\  转义字符，将特殊字符转为常字符 \\ --> \ (代表'\'的字面意义)

[] 表示字符的集合  [amk]      --> 'a', 'm' or 'k'
   '-'表示范围    [a-z]      --> 小写字母集合 
                 [0-5][0-9] --> 00 to 59
                 [0-9A-Fa-f] --> 任何16进制数字
                 [a\-z] [-a] [a-] 中’-‘表示字面意思上的’-‘
                 [^5] --> 不包含5的字符串 [^^] --> '^'(’^'必须是第一个字符才有意义)
                 [()[\]{}] and []()[{}] --> 插入语(’]‘表示字面意思上的’]‘)

| 或 A|B --> 'A' or 'b'

(...) 匹配括号内的正则表达式 或者 表示组的开始和结束（租的内容在匹配后可被检索）

\d 匹配一个数字字符。等价于 [0-9]。
\D 匹配一个非数字字符。等价于 [^0-9]。
\s 匹配任何空白字符，包括空格、制表符、换页符等等。等价于 [ \f\n\r\t\v]。
\S 匹配任何非空白字符。等价于 [^ \f\n\r\t\v]。
\w 匹配包括下划线的任何单词字符。等价于'[A-Za-z0-9_]'。
\W 匹配任何非单词字符。等价于 '[^A-Za-z0-9_]'。
```
待续 ...

## docker使用

**安装**

```shell
wget -qO- https://get.docker.com/ | sh
```
>If you would like to use Docker as a non-root user, you should now consider
adding your user to the "docker" group with something like:
```
 sudo usermod -aG docker runoob
```
>Remember that you will have to log out and back in for this to take effect!   
当要以非root用户可以直接运行docker时，需要执行 sudo usermod -aG docker runoob 命令，然后重新登陆，否则会有如下报错

**启动**

```bash
sudo service docker start
```

>docker:查看使用帮助  
docker COMMAND --help: 查看命令具体使用方法

## Caffe预处理操作

### caffe编译

1. 将caffe根目录下的python文件夹加入到环境变量

>打开配置文件bashrc  
```bash
sudo vi ~/.bashrc  
```
>在后边加入  
```
export PYTHONPATH=/home/xxx/caffe/python:$PYTHONPATH
```
>保存，更新配置文件
```bash
sudo ldconfig
# 也可以使用
source ~/.bashrc
```
2. 编辑配置文件Makefile.config

```bash
## Refer to http://caffe.berkeleyvision.org/installation.html
# Contributions simplifying and improving our build system are welcome!

# 使用cuDNN加速时配置
# cuDNN acceleration switch (uncomment to build with cuDNN).
USE_CUDNN := 1

# 如果没有GPU,使用CPU配置时取消注释
# CPU-only switch (uncomment to build without GPU support).
# CPU_ONLY := 1

# uncomment to disable IO dependencies and corresponding data layers
# USE_OPENCV := 0
# USE_LEVELDB := 0
# USE_LMDB := 0

# uncomment to allow MDB_NOLOCK when reading LMDB files (only if necessary)
#    You should not set this flag if you will be reading LMDBs with any
#    possibility of simultaneous read and write
# ALLOW_LMDB_NOLOCK := 1

# Uncomment if you're using OpenCV 3
# OPENCV_VERSION := 3

# To customize your choice of compiler, uncomment and set the following.
# N.B. the default for Linux is g++ and the default for OSX is clang++
# CUSTOM_CXX := g++

# CUDA目录 包含bin/ 和 lib/ 目录
# CUDA directory contains bin/ and lib/ directories that we need.
CUDA_DIR := /usr/local/cuda
# On Ubuntu 14.04, if cuda tools are installed via
# "sudo apt-get install nvidia-cuda-toolkit" then use this instead:
# CUDA_DIR := /usr

# CUDA architecture setting: going with all of them.
# For CUDA < 6.0, comment the *_50 lines for compatibility.
CUDA_ARCH := -gencode arch=compute_20,code=sm_20 \
        -gencode arch=compute_20,code=sm_21 \
        -gencode arch=compute_30,code=sm_30 \
        -gencode arch=compute_35,code=sm_35 \
        -gencode arch=compute_50,code=sm_50 \
        -gencode arch=compute_50,code=compute_50

# BLAS choice:
# atlas for ATLAS (default)
# mkl for MKL
# open for OpenBlas
BLAS := atlas
# Custom (MKL/ATLAS/OpenBLAS) include and lib directories.
# Leave commented to accept the defaults for your choice of BLAS
# (which should work)!
# BLAS_INCLUDE := /path/to/your/blas
# BLAS_LIB := /path/to/your/blas

# Homebrew puts openblas in a directory that is not on the standard search path
# BLAS_INCLUDE := $(shell brew --prefix openblas)/include
# BLAS_LIB := $(shell brew --prefix openblas)/lib

# This is required only if you will compile the matlab interface.
# MATLAB directory should contain the mex binary in /bin.
# MATLAB_DIR := /usr/local
# MATLAB_DIR := /Applications/MATLAB_R2012b.app

# NOTE: this is required only if you will compile the python interface.
# We need to be able to find Python.h and numpy/arrayobject.h.

# 是要使用系统自带的python还是使用anaconda的环境

# PYTHON_INCLUDE := /usr/include/python2.7 \
        /usr/lib/python2.7/dist-packages/numpy/core/include
# Anaconda Python distribution is quite popular. Include path:
# Verify anaconda location, sometimes it's in root.
ANACONDA_HOME := $(HOME)/anaconda2
PYTHON_INCLUDE := $(ANACONDA_HOME)/include \
        $(ANACONDA_HOME)/include/python2.7 \
        $(ANACONDA_HOME)/lib/python2.7/site-packages/numpy/core/include \

# We need to be able to find libpythonX.X.so or .dylib.
# PYTHON_LIB := /usr/lib
PYTHON_LIB := $(ANACONDA_HOME)/lib

# Homebrew installs numpy in a non standard path (keg only)
# PYTHON_INCLUDE += $(dir $(shell python -c 'import numpy.core; print(numpy.core.__file__)'))/include
# PYTHON_LIB += $(shell brew --prefix numpy)/lib

# Uncomment to support layers written in Python (will link against Python libs)
WITH_PYTHON_LAYER := 1

# Whatever else you find you need goes here.
INCLUDE_DIRS := $(PYTHON_INCLUDE) /usr/local/include
LIBRARY_DIRS := $(PYTHON_LIB) /usr/local/lib /usr/lib

# If Homebrew is installed at a non standard location (for example your home directory) and you use it for general dependencies
# INCLUDE_DIRS += $(shell brew --prefix)/include
# LIBRARY_DIRS += $(shell brew --prefix)/lib

# Uncomment to use `pkg-config` to specify OpenCV library paths.
# (Usually not necessary -- OpenCV libraries are normally installed in one of the above $LIBRARY_DIRS.)
# USE_PKG_CONFIG := 1

BUILD_DIR := build
DISTRIBUTE_DIR := distribute

# Uncomment for debugging. Does not work on OSX due to https://github.com/BVLC/caffe/issues/171
# DEBUG := 1

# The ID of the GPU that 'make runtest' will use to run unit tests.
TEST_GPUID := 0

# enable pretty build (comment to see full commands)
Q ?= @
```
>编译：
```bash
# 防止错误 编译
sudo make test -j8
sudo make runtest -j8
# 正式编译
sudo make pycaffe
```
>也许你在编译runtest的时候，会报这样的错误：
```python
.build_release/test/test_all.testbin: error while loading shared libraries: libhdf5.so.10: cannot open shared object file: No such file or directory
```
>这是因为 libhdf5.so的版本问题，你可以进入/usr/lib/x86_64-linux-gnu看一下，你的libhdf5.so.x中的那个x是多少，比如我的是libhdf5.so.7  
 
>可以执行下面几行代码解决:

```bash
cd /usr/lib/x86_64-linux-gnu
sudo ln -s libhdf5.so.7 libhdf5.so.10
sudo ln -s libhdf5_hl.so.7 libhdf5_hl.so.10
sudo ldconfig
```

>进入python环境，测试接口是否编译成功

```python
# python
>>> import caffe
```
若没有提示错误，说明编译成功

### caffe数据生成
**图像数据转换成db(leveldb/lmdb)文件**

>在caffe中，作者为我们提供了这样一个文件：convert_imageset.cpp，存放在根目录下的tools文件夹下。编译之后，生成对应的可执行文件放在 buile/tools/ 下面，这个文件的作用就是用于将图片文件转换成caffe框架中能直接使用的db文件  

>**使用格式：**

```python
 convert_imageset [FLAGS] ROOTFOLDER/ LISTFILE DB_NAME

# 四个参数：

# FLAGS: 图片参数组，后面详细介绍
# ROOTFOLDER/: 图片存放的绝对路径，从linux系统根目录开始
# LISTFILE: 图片文件列表清单，一般为一个txt文件，一行一张图片
# DB_NAME: 最终生成的db文件存放目录
```

>创建一个sh脚本文件，调用linux命令来生成图片清单(以caffe自带图片为例)

```bash
sudo vi examples/images/create_filelist.sh
```
```sh
# /usr/bin/env sh
DATA=examples/images
echo "Create train.txt..."
rm -rf $DATA/train.txt
find $DATA -name *cat.jpg | cut -d '/' -f3 | sed "s/$/ 1/">>$DATA/train.txt
find $DATA -name *bike.jpg | cut -d '/' -f3 | sed "s/$/ 2/">>$DATA/tmp.txt
cat $DATA/tmp.txt>>$DATA/train.txt
rm -rf $DATA/tmp.txt
echo "Done.."
```
>output:`train.txt`
```
cat.jpg 1
fish-bike.jpg 2
```
### 图片数据均值的计算
>为了提高提高速度和精度，一般都将图片减去均值后，再进行训练和测试

#### 1. 二进制格式的均值计算
>caffe中使用的均值数据格式是binaryproto, 作者为我们提供了一个计算均值的文件compute_image_mean.cpp，放在caffe根目录下的tools文件夹里面。编译后的可执行体放在 build/tools/ 下面，我们直接调用就可以了
```bash
sudo build/tools/compute_image_mean examples/mnist/mnist_train_lmdb examples/mnist/mean.binaryproto

# 两个参数：
# 第一个参数：examples/mnist/mnist_train_lmdb， 表示需要计算均值的数据，格式为lmdb的训练数据。
# 第二个参数：examples/mnist/mean.binaryproto， 计算出来的结果保存文件。
```

#### 2. python格式的均值计算
>如果我们要使用python接口，或者我们要进行特征可视化，可能就要用到python格式的均值文件了。首先，我们用lmdb格式的数据，计算出二进制格式的均值，然后，再转换成python格式的均值。

>可以编写一个脚本来实现：
```python
#!/usr/bin/env python
import numpy as np
import sys,caffe

if len(sys.argv)!=3:
    print "Usage: python convert_mean.py mean.binaryproto mean.npy"
    sys.exit()

blob = caffe.proto.caffe_pb2.BlobProto()
bin_mean = open( sys.argv[1] , 'rb' ).read()
blob.ParseFromString(bin_mean)
arr = np.array( caffe.io.blobproto_to_array(blob) )
npy_mean = arr[0]
np.save( sys.argv[2] , npy_mean )

# 将脚本保存为 convert_mean.py

#调用格式为：
# sudo python convert_mean.py mean.binaryproto mean.npy
# 其中的 mean.binaryproto 就是经过前面步骤计算出来的二进制均值。
# mean.npy就是我们需要的python格式的均值。
```

## caffe训练

>一. 写配置文件
```python
# -*- coding: utf-8 -*-
"""
Spyder Editor

"""

from caffe import layers as L,params as P,to_proto
path='/home/xxx/data/'                    #保存数据和配置文件的路径
train_lmdb=path+'train_db'                #训练数据LMDB文件的位置
val_lmdb=path+'val_db'                    #验证数据LMDB文件的位置
mean_file=path+'mean.binaryproto'         #均值文件的位置
train_proto=path+'train.prototxt'         #生成的训练配置文件保存的位置
val_proto=path+'val.prototxt'             #生成的验证配置文件保存的位置
#编写一个函数，用于生成网络
def create_net(lmdb,batch_size,include_acc=False):
    #创建第一层：数据层。向上传递两类数据：图片数据和对应的标签
    data, label = L.Data(source=lmdb, backend=P.Data.LMDB, batch_size=batch_size, ntop=2,
        transform_param=dict(crop_size=40,mean_file=mean_file,mirror=True))
    #创建第二屋：卷积层
    conv1=L.Convolution(data, kernel_size=5, stride=1,num_output=16, pad=2,weight_filler=dict(type='xavier'))
    #创建激活函数层
    relu1=L.ReLU(conv1, in_place=True)
    #创建池化层
    pool1=L.Pooling(relu1, pool=P.Pooling.MAX, kernel_size=3, stride=2)
    conv2=L.Convolution(pool1, kernel_size=3, stride=1,num_output=32, pad=1,weight_filler=dict(type='xavier'))
    relu2=L.ReLU(conv2, in_place=True)
    pool2=L.Pooling(relu2, pool=P.Pooling.MAX, kernel_size=3, stride=2)
    #创建一个全连接层
    fc3=L.InnerProduct(pool2, num_output=1024,weight_filler=dict(type='xavier'))
    relu3=L.ReLU(fc3, in_place=True)
    #创建一个dropout层
    drop3 = L.Dropout(relu3, in_place=True)
    fc4 = L.InnerProduct(drop3, num_output=10,weight_filler=dict(type='xavier'))
    #创建一个softmax层
    loss = L.SoftmaxWithLoss(fc4, label)
    
    if include_acc:             #在训练阶段，不需要accuracy层，但是在验证阶段，是需要的
        acc = L.Accuracy(fc4, label)
        return to_proto(loss, acc)
    else:
        return to_proto(loss)
    
def write_net():
    #将以上的设置写入到prototxt文件
    with open(train_proto, 'w') as f:
        f.write(str(create_net(train_lmdb,batch_size=64)))

    #写入配置文件    
    with open(val_proto, 'w') as f:
        f.write(str(create_net(val_lmdb,batch_size=32, include_acc=True)))
        
if __name__ == '__main__':
    write_net()
```
>通过上面这个文件的执行，我们就会得到两个配置文件：train.prototxt和val.prototxt，分别用于训练阶段和验证阶段。

>这种方式生成配置文件，必须有个前提，就是要先把原始图片转换成LMDB文件才行。如果我们已经把原始图片做成了一个列表清单（txt文件，一行一张图片），则可以不用LMDB格式作为输入数据，可以用ImageData作为数据源输入，代码如下：
```python
# -*- coding: utf-8 -*-

from caffe import layers as L,params as P,to_proto
path='/home/xxx/data/'
train_list=path+'train.txt'
val_list=path+'val.txt'           
train_proto=path+'train.prototxt'   
val_proto=path+'val.prototxt'       

def create_net(img_list,batch_size,include_acc=False):
    data,label=L.ImageData(source=img_list,batch_size=batch_size,new_width=48,new_height=48,ntop=2,
                           transform_param=dict(crop_size=40,mirror=True))

    conv1=L.Convolution(data, kernel_size=5, stride=1,num_output=16, pad=2,weight_filler=dict(type='xavier'))
    relu1=L.ReLU(conv1, in_place=True)
    pool1=L.Pooling(relu1, pool=P.Pooling.MAX, kernel_size=3, stride=2)
    conv2=L.Convolution(pool1, kernel_size=53, stride=1,num_output=32, pad=1,weight_filler=dict(type='xavier'))
    relu2=L.ReLU(conv2, in_place=True)
    pool2=L.Pooling(relu2, pool=P.Pooling.MAX, kernel_size=3, stride=2)
    conv3=L.Convolution(pool2, kernel_size=53, stride=1,num_output=32, pad=1,weight_filler=dict(type='xavier'))
    relu3=L.ReLU(conv3, in_place=True)
    pool3=L.Pooling(relu3, pool=P.Pooling.MAX, kernel_size=3, stride=2)
    fc4=L.InnerProduct(pool3, num_output=1024,weight_filler=dict(type='xavier'))
    relu4=L.ReLU(fc4, in_place=True)
    drop4 = L.Dropout(relu4, in_place=True)
    fc5 = L.InnerProduct(drop4, num_output=7,weight_filler=dict(type='xavier'))
    loss = L.SoftmaxWithLoss(fc5, label)
    
    if include_acc:             
        acc = L.Accuracy(fc5, label)
        return to_proto(loss, acc)
    else:
        return to_proto(loss)
    
def write_net():
    #
    with open(train_proto, 'w') as f:
        f.write(str(create_net(train_list,batch_size=64)))

    #    
    with open(val_proto, 'w') as f:
        f.write(str(create_net(val_list,batch_size=32, include_acc=True)))
        
if __name__ == '__main__':
    write_net()
```
>二. 生成sover文件
> caffe在训练的时候，需要一些参数设置，我们一般将这些参数设置在一个叫solver.prototxt的文件里面，如下：
```
base_lr: 0.001
display: 782
gamma: 0.1
lr_policy: “step”
max_iter: 78200
momentum: 0.9
snapshot: 7820
snapshot_prefix: “snapshot”
solver_mode: GPU
solver_type: SGD
stepsize: 26067
test_interval: 782
test_iter: 313
test_net: “/home/xxx/data/val.prototxt”
train_net: “/home/xxx/data/proto/train.prototxt”
weight_decay: 0.0005
```
>有一些参数需要计算的，也不是乱设置。  
假设我们有50000个训练样本，batch_size为64，即每批次处理64个样本，那么需要迭代50000/64=782次才处理完一次全部的样本。我们把处理完一次所有的样本，称之为一代，即epoch。所以，这里的test_interval设置为782，即处理完一次所有的训练数据后，才去进行测试。如果我们想训练100代，则需要设置max_iter为78200.  
同理，如果有10000个测试样本，batch_size设为32，那么需要迭代10000/32=313次才完整地测试完一次，所以设置test_iter为313.  
学习率变化规律我们设置为随着迭代次数的增加，慢慢变低。总共迭代78200次，我们将变化lr_rate三次，所以stepsize设置为78200/3=26067，即每迭代26067次，我们就降低一次学习率。 

生成solver文件:
```python
# -*- coding: utf-8 -*-

path='/home/xxx/data/'
solver_file=path+'solver.prototxt'     #solver文件保存位置

sp={}
sp['train_net']=‘“’+path+'train.prototxt”'  # 训练配置文件
sp['test_net']=‘“’+path+'val.prototxt”'     # 测试配置文件
sp['test_iter']='313'                  # 测试迭代次数
sp['test_interval']='782'              # 测试间隔
sp['base_lr']='0.001'                  # 基础学习率
sp['display']='782'                    # 屏幕日志显示间隔
sp['max_iter']='78200'                 # 最大迭代次数
sp['lr_policy']='“step”'                 # 学习率变化规律
sp['gamma']='0.1'                      # 学习率变化指数
sp['momentum']='0.9'                   # 动量
sp['weight_decay']='0.0005'            # 权值衰减
sp['stepsize']='26067'                 # 学习率变化频率
sp['snapshot']='7820'                   # 保存model间隔
sp['snapshot_prefix']=‘"snapshot"’       # 保存的model前缀
sp['solver_mode']='GPU'                # 是否使用gpu
sp['solver_type']='SGD'                # 优化算法

def write_solver():
    #写入文件
    with open(solver_file, 'w') as f:
        for key, value in sorted(sp.items()):
            if not(type(value) is str):
                raise TypeError('All solver parameters must be strings')
            f.write('%s: %s\n' % (key, value))
if __name__ == '__main__':
    write_solver()
```

### 模型训练
>如果不进行可视化，只想得到一个最终的训练model, 那么代码非常简单，如下 :
```python
import caffe
caffe.set_device(0)
caffe.set_mode_gpu()
solver = caffe.SGDSolver('/home/xxx/data/solver.prototxt')
solver.solve()
```

## ubuntu 18.04 

>Top Panel Workspace Scroll  
overflow Alt-Tab  
Dynamic Top Bar  
topicons plus 状态栏  
Screenshot tools 截图

- Arc Theme
- Flatabulous
- Arc-Flatabulous Theme
- OSX-Arc-Collection

>以上是几款扁平化的 Gnome Shell 主题，Arc-Flatabulous Theme 与 Arc Theme 相比具有更 加漂亮的窗口按钮。从名字即可看出 Arc-Flatabulous 是 Arc 和 Flatabulous 的结合。这三款 主题都托管于 GitHub： Arc Theme， Flatabulous， Arc-Flatabulous Theme。

### 安装 adobe reader
[How to Install Adobe Acrobat Reader on Ubuntu 18.04 Bionic Beaver Linux](https://linuxconfig.org/how-to-install-adobe-acrobat-reader-on-ubuntu-18-04-bionic-beaver-linux)

```bash 18.04
sudo apt install libxml2:i386 gdebi-core

sudo wget ftp://ftp.adobe.com/pub/adobe/reader/unix/9.x/9.5.5/enu/AdbeRdr9.5.5-1_i386linux_enu.deb

sudo dpkg -i AdbeRdr9.5.5-1_i386linux_enu.deb
```
```bash 16.04
sudo add-apt-repository "deb http://archive.canonical.com/ precise partner"
sudo apt-get update
sudo apt install adobereader-enu
# remove 
sudo add-apt-repository -r "deb http://archive.canonical.com/ precise partner"
sudo apt update
```

## 课程学习

### python机器学习

## PCA降维
[再谈协方差矩阵之主成分分析](http://pinkyjie.com/2011/02/24/covariance-pca/)

Anaconda

jupyter: 打开ipthon文件

## one-hot 编码

``` python
from sklearn import preprocessing
enc = preprocessing.OneHotEncoder()
enc.fit([[0,0,3],[1,1,0],[0,2,1],[1,0,2]])
array = enc.transform([[0,1,3]]).toarray()
print "enc.n_values_ is:",enc.n_values_
print "enc.feature_indices_ is:",enc.feature_indices_
print array
```

```python
enc.n_values_ is: [2 3 4]
enc.feature_indices_ is: [0 2 5 9]
[[ 1.  0.  0.  1.  0.  0.  1.  0.  0.]]
```

## activemq启动
>启动
进入%ACTIVEMQ_HOME%\bin目录下：
```bash
./activemq start
```

```
127.0.0.1:8161 控制台
默认账号密码均为 admin
可在%ACTIVEMQ_HOME%\conf中jetty-realm.properties设置账号密码。
```